# Ejercicio 5: Observabilidad MÃ­nima Viable - API REST Observable

**Autor:** Ejercicio 5 - Semana 7 IA  
**Fecha:** 26 Nov 2025  
**Tema:** Logging estructurado, mÃ©tricas, traces y dashboards para producciÃ³n

---

## ðŸ“‹ TABLA DE CONTENIDOS

1. [DescripciÃ³n General](#-descripciÃ³n-general)
2. [Arquitectura](#-arquitectura)
3. [Quickstart](#-quickstart)
4. [Features Implementadas](#-features-implementadas)
5. [Estructura del Proyecto](#-estructura-del-proyecto)
6. [Logging Estructurado](#-logging-estructurado)
7. [MÃ©tricas y KPIs](#-mÃ©tricas-y-kpis)
8. [Request Tracing](#-request-tracing)
9. [Dashboard Live](#-dashboard-live)
10. [Alertas y Umbrales](#-alertas-y-umbrales)
11. [Troubleshooting](#-troubleshooting)
12. [ProducciÃ³n](#-producciÃ³n)

---

## ðŸŽ¯ DESCRIPCIÃ“N GENERAL

Este ejercicio implementa **observabilidad mÃ­nima viable (MVo)** para una API REST, proporcionando las herramientas esenciales para monitorear, debuggear y mantener una API en producciÃ³n.

### Objetivos de Aprendizaje

1. **Structured Logging:** Logs en formato JSON para queries eficientes
2. **Metrics Collection:** Latencia (p50/p95/p99) y error rates por endpoint
3. **Request Tracing:** Pipeline completo requestâ†’response con timestamps
4. **Dashboard:** VisualizaciÃ³n de mÃ©tricas clave en tiempo real
5. **Alerting:** Umbrales crÃ­ticos para detecciÃ³n temprana de problemas

### Principios de Observabilidad

**Los 3 Pilares:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OBSERVABILITY = Logs + Metrics + Traces                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ LOGS      â†’  "QuÃ© pasÃ³" (events, errors)               â”‚
â”‚ METRICS   â†’  "CÃ³mo estÃ¡ el sistema" (latency, errors)  â”‚
â”‚ TRACES    â†’  "Por quÃ© es lento" (request pipeline)     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ—ï¸ ARQUITECTURA

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT (Browser/Mobile)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP Request
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OBSERVABILITY MIDDLEWARE                      â”‚
â”‚  1. Generate correlationId                                  â”‚
â”‚  2. Create RequestTracer                                    â”‚
â”‚  3. Log "Request started"                                   â”‚
â”‚  4. Measure latency (start timer)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENDPOINT (CRUD)                          â”‚
â”‚  tracer.add_step("validating_input")                        â”‚
â”‚  tracer.add_step("querying_database")                       â”‚
â”‚  tracer.add_step("product_created")                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OBSERVABILITY MIDDLEWARE                      â”‚
â”‚  5. Calculate latency (stop timer)                          â”‚
â”‚  6. Record metrics (MetricsCollector)                       â”‚
â”‚  7. Log "Request completed"                                 â”‚
â”‚  8. Add headers (X-Correlation-Id, X-Latency-Ms)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP Response
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OBSERVABILITY STACK                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ StructuredLoggerâ”‚ MetricsCollectorâ”‚ RequestTracerâ”‚     â”‚
â”‚  â”‚                â”‚  â”‚              â”‚  â”‚              â”‚     â”‚
â”‚  â”‚ â€¢ JSON logs    â”‚  â”‚ â€¢ Latencies  â”‚  â”‚ â€¢ Steps      â”‚     â”‚
â”‚  â”‚ â€¢ correlationIdâ”‚  â”‚ â€¢ Error ratesâ”‚  â”‚ â€¢ Timestamps â”‚     â”‚
â”‚  â”‚ â€¢ ISO 8601 ts  â”‚  â”‚ â€¢ P50/95/99  â”‚  â”‚ â€¢ Details    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚           â”‚                 â”‚                  â”‚             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                             â”‚                                â”‚
â”‚                             â–¼                                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚ API Endpoints  â”‚                        â”‚
â”‚                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
â”‚                    â”‚ GET /_metrics  â”‚ â† Dashboard queries    â”‚
â”‚                    â”‚ GET /_trace    â”‚ â† Debugging            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ QUICKSTART

### Pre-requisitos

- Python 3.11+
- FastAPI, Uvicorn, Pydantic (ver requirements.txt)

### InstalaciÃ³n

```bash
cd semana7_ia/ejercicio5

# Instalar dependencias
pip install fastapi uvicorn pydantic requests
```

### EjecuciÃ³n RÃ¡pida

```bash
# Terminal 1: Levantar API observable
uvicorn api_observable:app --reload --port 8000

# Output esperado:
# INFO:     Uvicorn running on http://127.0.0.1:8000
# {"ts":"2025-11-26T13:42:15Z","level":"INFO","msg":"API starting up","service":"api_observable","version":"1.0.0"}
```

```bash
# Terminal 2: Generar trÃ¡fico de prueba
curl -X POST http://localhost:8000/api/v1/products \
  -H "Content-Type: application/json" \
  -H "X-User-Id: user123" \
  -d '{"name": "iPhone 15 Pro", "price": 999, "currency": "USD"}'

# Ver logs estructurados en Terminal 1:
# {"ts":"...","level":"INFO","msg":"Request started","correlationId":"550e8400-...","method":"POST","path":"/api/v1/products","userId":"user123"}
# {"ts":"...","level":"INFO","msg":"Product created","correlationId":"550e8400-...","product_id":"a1b2c3d4-...","name":"iPhone 15 Pro"}
# {"ts":"...","level":"INFO","msg":"Request completed","correlationId":"550e8400-...","status":201,"latency_ms":12.34}
```

```bash
# Terminal 3: Dashboard en tiempo real
python dashboard_live.py

# Output:
# ================================================================================
# ðŸ” OBSERVABILITY DASHBOARD - LIVE METRICS
# ================================================================================
# ðŸ“Š GLOBAL METRICS
# Total Requests:  15
# Throughput:      0.25 req/s
# 
# â±ï¸  LATENCY & ERROR RATE PER ENDPOINT
# POST /api/v1/products    12.3ms  24.5ms  42.1ms  2.5%   0.0%
```

---

## âœ¨ FEATURES IMPLEMENTADAS

### 1. âœ… Logging Estructurado JSON

**CaracterÃ­sticas:**
- Formato JSON por lÃ­nea (parseable con `jq`, Elasticsearch)
- Timestamps ISO 8601 con timezone UTC
- correlationId en TODAS las logs de la misma request
- Campos estÃ¡ndar: ts, level, logger, msg, correlationId, path, method, status, latency_ms, userId

**Ejemplo de log:**
```json
{
  "ts": "2025-11-26T13:42:15.123456Z",
  "level": "INFO",
  "logger": "api_observable",
  "msg": "Request completed",
  "correlationId": "550e8400-e29b-41d4-a716-446655440000",
  "method": "POST",
  "path": "/api/v1/products",
  "status": 201,
  "latency_ms": 12.34,
  "userId": "user123"
}
```

---

### 2. âœ… MÃ©tricas de Latencia (P50, P95, P99)

**CaracterÃ­sticas:**
- Percentiles calculados por endpoint
- Historial de Ãºltimas 1000 requests por endpoint
- P50 (mediana), P95, P99 para identificar tail latency
- Contador de requests totales

**Query API:**
```bash
curl http://localhost:8000/api/v1/_metrics | jq '.data["POST /api/v1/products"].latency'

# Output:
# {
#   "p50": 12.34,
#   "p95": 24.56,
#   "p99": 42.12,
#   "count": 150
# }
```

---

### 3. âœ… Error Rate por Endpoint (4xx y 5xx)

**CaracterÃ­sticas:**
- Tasa de error 4xx (client errors) y 5xx (server errors) separadas
- Contador de errores totales por status code
- CÃ¡lculo de porcentaje sobre total de requests

**Query API:**
```bash
curl http://localhost:8000/api/v1/_metrics | jq '.data["GET /api/v1/products/550e8400-e29b-41d4-a716-446655440000"].errors'

# Output:
# {
#   "error_rate_4xx": 8.0,
#   "error_rate_5xx": 0.1,
#   "total": 500,
#   "errors_4xx": 40,
#   "errors_5xx": 1
# }
```

---

### 4. âœ… Request Tracing (Pipeline Steps)

**CaracterÃ­sticas:**
- Trace completo de pipeline requestâ†’response
- Timestamps relativos (ms desde inicio de request)
- Detalles customizados por paso (ej: product_id, name)
- Accesible en runtime via `/_trace` endpoint

**Ejemplo de trace:**
```bash
curl http://localhost:8000/api/v1/_trace | jq '.data.steps'

# Output:
# [
#   {
#     "step": "request_received",
#     "timestamp_ms": 0.12,
#     "details": {"method": "POST", "path": "/api/v1/products"}
#   },
#   {
#     "step": "validating_input",
#     "timestamp_ms": 3.45,
#     "details": {"name": "iPhone 15 Pro"}
#   },
#   {
#     "step": "product_created",
#     "timestamp_ms": 8.76,
#     "details": {"product_id": "a1b2c3d4-..."}
#   }
# ]
```

---

### 5. âœ… Dashboard CLI en Tiempo Real

**CaracterÃ­sticas:**
- ActualizaciÃ³n cada 5 segundos
- MÃ©tricas globales (total requests, uptime, throughput)
- Latencia por endpoint con barras visuales
- Colores ANSI (verde/amarillo/rojo segÃºn umbrales)
- Alertas activas destacadas

**Ejecutar:**
```bash
python dashboard_live.py
```

---

### 6. âœ… 3 Alertas con Umbrales CrÃ­ticos

| Alerta | Umbral | Criticidad |
|--------|--------|------------|
| **High 5xx Rate** | >1% | ðŸ”´ CRITICAL |
| **High Latency** | P95 >50ms | âš ï¸ WARNING |
| **High 4xx Rate (POST)** | >15% | âš ï¸ WARNING |

---

## ðŸ“ ESTRUCTURA DEL PROYECTO

```
ejercicio5/
â”‚
â”œâ”€â”€ api_observable.py           # ðŸŽ¯ API con observabilidad completa (650 lÃ­neas)
â”‚   â”œâ”€â”€ StructuredLogger        # Logging JSON con custom formatter
â”‚   â”œâ”€â”€ MetricsCollector         # Latencia + error rate por endpoint
â”‚   â”œâ”€â”€ RequestTracer            # Pipeline tracing con timestamps
â”‚   â”œâ”€â”€ observability_middleware # InstrumentaciÃ³n automÃ¡tica
â”‚   â””â”€â”€ Endpoints:
â”‚       â”œâ”€â”€ CRUD /api/v1/products (mismos del Ejercicio 4)
â”‚       â”œâ”€â”€ GET /api/v1/_metrics  (mÃ©tricas agregadas)
â”‚       â”œâ”€â”€ GET /api/v1/_trace    (trace de request actual)
â”‚       â”œâ”€â”€ POST /_test/clear     (limpieza para tests)
â”‚       â””â”€â”€ GET /_test/stats      (estadÃ­sticas DB)
â”‚
â”œâ”€â”€ dashboard_live.py            # ðŸ“Š Dashboard CLI interactivo (150 lÃ­neas)
â”‚   â”œâ”€â”€ render_dashboard()       # Loop cada 5s
â”‚   â”œâ”€â”€ MÃ©tricas globales        # Total requests, uptime, throughput
â”‚   â”œâ”€â”€ Latencia por endpoint    # P50/P95/P99 con barras ASCII
â”‚   â””â”€â”€ Alertas activas          # ðŸ”´/âš ï¸ segÃºn umbrales
â”‚
â”œâ”€â”€ DASHBOARD.md                 # ðŸ“ˆ EspecificaciÃ³n de 5 grÃ¡ficos + 3 alertas
â”‚   â”œâ”€â”€ GrÃ¡fico 1: Latency Percentiles (P50/P95/P99)
â”‚   â”œâ”€â”€ GrÃ¡fico 2: Error Rate 4xx/5xx
â”‚   â”œâ”€â”€ GrÃ¡fico 3: Request Throughput (req/s)
â”‚   â”œâ”€â”€ GrÃ¡fico 4: Request Tracing (pipeline steps)
â”‚   â”œâ”€â”€ GrÃ¡fico 5: Active Errors Stream (Ãºltimos 10)
â”‚   â””â”€â”€ 3 Alertas: High 5xx, High Latency, High 4xx (POST)
â”‚
â”œâ”€â”€ EJEMPLOS_LOGS.md             # ðŸ“ Logs reales con anÃ¡lisis (500 lÃ­neas)
â”‚   â”œâ”€â”€ LOG 1: Request exitosa (201 Created)
â”‚   â”œâ”€â”€ LOG 2: Error 404 (Product Not Found)
â”‚   â”œâ”€â”€ LOG 3: Error 409 (Version Conflict)
â”‚   â”œâ”€â”€ LOG 4: Error 422 (Validation Error)
â”‚   â”œâ”€â”€ LOG 5: Error 500 (Internal Server Error)
â”‚   â”œâ”€â”€ LOG 6: Lifecycle events (startup/shutdown)
â”‚   â”œâ”€â”€ Queries Ãºtiles con jq
â”‚   â””â”€â”€ Retention policy + sensitive data policy
â”‚
â”œâ”€â”€ CRITICA_Y_MEJORA.md          # ðŸ” AnÃ¡lisis tÃ©cnico (800 lÃ­neas)
â”‚   â”œâ”€â”€ Fortalezas (5 secciones)
â”‚   â”œâ”€â”€ Debilidades (7 secciones)
â”‚   â”œâ”€â”€ Prompt mejorado v2 (production-grade)
â”‚   â””â”€â”€ Roadmap de implementaciÃ³n (5 fases)
â”‚
â””â”€â”€ README.md                    # ðŸ“– Este archivo
    â””â”€â”€ DocumentaciÃ³n completa
```

---

## ðŸ“ LOGGING ESTRUCTURADO

### ImplementaciÃ³n

**StructuredLogger:**
```python
class StructuredLogger:
    def _json_formatter(self):
        class JSONFormatter(logging.Formatter):
            def format(self, record):
                return json.dumps({
                    "ts": datetime.utcnow().isoformat() + "Z",
                    "level": record.levelname,
                    "msg": record.getMessage(),
                    "correlationId": getattr(record, 'correlationId', None),
                    "path": getattr(record, 'path', None),
                    # ... mÃ¡s campos
                })
```

### Uso en Endpoints

```python
@app.post("/api/v1/products")
def create_product(data: ProductCreate, request: Request):
    logger.info(
        "Product created",
        correlationId=request.state.correlation_id,
        product_id=str(product.id),
        name=product.name
    )
```

### Queries con jq

```bash
# 1. Filtrar solo errores
tail -f logs/api.log | jq 'select(.status >= 400)'

# 2. Contar errores por status code
cat logs/api.log | jq -s 'group_by(.status) | map({status: .[0].status, count: length})'

# 3. Requests de un usuario
cat logs/api.log | jq 'select(.userId == "user123")'

# 4. Latencia promedio por endpoint
cat logs/api.log | jq -s 'group_by(.path) | map({endpoint: .[0].path, avg_latency: (map(.latency_ms) | add / length)})'

# 5. Trazar request por correlationId
cat logs/api.log | jq 'select(.correlationId == "550e8400-...")'
```

Ver mÃ¡s ejemplos en **EJEMPLOS_LOGS.md**.

---

## ðŸ“Š MÃ‰TRICAS Y KPIs

### MetricsCollector

**Estructura de datos:**
```python
class MetricsCollector:
    _latencies: Dict[str, deque]       # endpoint â†’ [latency_ms, ...]
    _error_counts: Dict[str, Dict[int, int]]  # endpoint â†’ {status_code: count}
    _request_counts: Dict[str, int]    # endpoint â†’ total_requests
```

### MÃ©tricas Disponibles

1. **Latency Percentiles:**
   ```python
   {
     "p50": 12.34,  # 50% de requests son mÃ¡s rÃ¡pidas
     "p95": 24.56,  # 95% de requests son mÃ¡s rÃ¡pidas (SLA tÃ­pico)
     "p99": 42.12,  # 99% de requests son mÃ¡s rÃ¡pidas (tail latency)
     "count": 150
   }
   ```

2. **Error Rates:**
   ```python
   {
     "error_rate_4xx": 2.5,  # % de requests con 4xx
     "error_rate_5xx": 0.1,  # % de requests con 5xx
     "total": 500,
     "errors_4xx": 13,
     "errors_5xx": 1
   }
   ```

3. **Global Metrics:**
   ```python
   {
     "total_requests": 12543,
     "uptime_seconds": 3600,
     "requests_per_second": 3.48
   }
   ```

### Query API

```bash
# Todas las mÃ©tricas
curl http://localhost:8000/api/v1/_metrics | jq

# Solo latencia de un endpoint
curl http://localhost:8000/api/v1/_metrics | \
  jq '.data["POST /api/v1/products"].latency'

# Endpoints con error rate > 5%
curl http://localhost:8000/api/v1/_metrics | \
  jq '.data | to_entries[] | select(.value.errors.error_rate_4xx > 5)'
```

---

## ðŸ” REQUEST TRACING

### RequestTracer

**ImplementaciÃ³n:**
```python
class RequestTracer:
    def __init__(self, correlation_id: str):
        self.correlation_id = correlation_id
        self.steps: List[Dict] = []
        self.start_time = time.perf_counter()
    
    def add_step(self, name: str, details: Optional[Dict] = None):
        elapsed_ms = (time.perf_counter() - self.start_time) * 1000
        self.steps.append({
            "step": name,
            "timestamp_ms": round(elapsed_ms, 2),
            "details": details or {}
        })
```

### Uso en Endpoints

```python
@app.get("/api/v1/products/{product_id}")
def get_product(product_id: uuid.UUID, request: Request):
    tracer = request.state.tracer
    
    tracer.add_step("querying_database", {"product_id": str(product_id)})
    product = db.get(product_id)
    
    if not product:
        tracer.add_step("product_not_found")
        raise HTTPException(status_code=404, ...)
    
    tracer.add_step("product_found", {"name": product.name})
    return {"data": product.model_dump(mode="json"), "error": None}
```

### Ver Trace

```bash
# Hacer request con header X-User-Id
curl -H "X-User-Id: user123" http://localhost:8000/api/v1/products | \
  jq '.data.items[0].id'

# Ver trace de esa request
curl http://localhost:8000/api/v1/_trace | jq '.data'

# Output:
# {
#   "correlationId": "550e8400-...",
#   "steps": [
#     {"step": "request_received", "timestamp_ms": 0.12, "details": {...}},
#     {"step": "querying_products_list", "timestamp_ms": 2.34, "details": {...}},
#     {"step": "products_retrieved", "timestamp_ms": 5.67, "details": {...}}
#   ]
# }
```

---

## ðŸ“ˆ DASHBOARD LIVE

### Ejecutar Dashboard

```bash
python dashboard_live.py
```

### Output Ejemplo

```
================================================================================
ðŸ” OBSERVABILITY DASHBOARD - LIVE METRICS
================================================================================
Timestamp: 2025-11-26 13:42:15
================================================================================

ðŸ“Š GLOBAL METRICS
--------------------------------------------------------------------------------
  Total Requests:      1,543
  Uptime:               45 min 23 sec
  Throughput:            0.57 req/s

â±ï¸  LATENCY & ERROR RATE PER ENDPOINT
--------------------------------------------------------------------------------
Endpoint                                      P50      P95      P99      4xx%     5xx%
--------------------------------------------------------------------------------
POST /api/v1/products                        12.3ms   24.5ms   42.1ms   2.5%    0.0%
  â””â”€ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 24.5ms / 100ms
GET /api/v1/products/550e8400-e29b-41d4...   3.1ms    8.2ms   14.5ms   8.0%    0.1%
  â””â”€ â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 8.2ms / 100ms

ðŸš¨ ACTIVE ALERTS
--------------------------------------------------------------------------------
  âš ï¸  HIGH 4xx RATE: GET /api/v1/products/... has 8.0% validation errors (threshold: 5%)

================================================================================
Refreshing in 5s... (Press Ctrl+C to exit)
```

### Features

- âœ… Auto-refresh cada 5 segundos
- âœ… Colores ANSI (verde/amarillo/rojo)
- âœ… Barras ASCII de latencia
- âœ… Alertas destacadas en secciÃ³n separada
- âœ… MÃ©tricas globales en header

---

## ðŸš¨ ALERTAS Y UMBRALES

### Alerta 1: High 5xx Rate

**DefiniciÃ³n:**
```python
if error_rate_5xx > 1.0:
    ALERT("CRITICAL: High server error rate")
```

**Umbral:** 1% de requests con 5xx

**JustificaciÃ³n:**
- 5xx = errores del servidor (bugs, DB down)
- 1% = ~10 errores cada 1000 requests
- CrÃ­tico porque afecta usuarios sin culpa de ellos

**AcciÃ³n:**
- ðŸ”” Notificar a on-call engineer
- ðŸ” Revisar logs: `tail -f logs/api.log | grep 'status":5'`
- ðŸ› ï¸ Rollback si deploy reciente
- ðŸ“Š Verificar health de DB/dependencies

---

### Alerta 2: High Latency (P95)

**DefiniciÃ³n:**
```python
if latency_p95 > 50.0:
    ALERT("WARNING: High latency detected")
```

**Umbral:** P95 > 50ms

**JustificaciÃ³n:**
- P95 = 95% de usuarios experimentan latencia < 50ms
- 50ms = umbral de "fast" segÃºn Google Web Vitals
- Si P95 > 50ms, 5% de usuarios tienen mala experiencia

**AcciÃ³n:**
- ðŸ“Š Ver trace: `curl http://localhost:8000/api/v1/_trace`
- ðŸ”Ž Identificar paso lento (DB query, external API)
- ðŸ—„ï¸ Revisar slow queries (si DB real)
- ðŸš€ Optimizar cÃ³digo del endpoint

---

### Alerta 3: High 4xx Rate on CREATE

**DefiniciÃ³n:**
```python
if endpoint == "POST /products" and error_rate_4xx > 15.0:
    ALERT("WARNING: High validation error rate on CREATE")
```

**Umbral:** 15% de requests con 4xx en POST /products

**JustificaciÃ³n:**
- POST /products = endpoint crÃ­tico
- 15% = 1 de cada 7 requests falla por validaciÃ³n
- Indica problemas en cliente (frontend)

**AcciÃ³n:**
- ðŸ“± Notificar a equipo frontend
- ðŸ“‹ Revisar Ãºltimos errores: `tail logs/api.log | grep 'POST.*products.*422'`
- ðŸ“ Documentar errores comunes en docs
- ðŸ”§ Mejorar mensajes de error

---

## ðŸ› TROUBLESHOOTING

### Problema 1: Dashboard No Se Conecta

**SÃ­ntoma:**
```
âŒ ERROR: No se pudo conectar a la API
Detalle: Connection refused
```

**SoluciÃ³n:**
```bash
# 1. Verificar que API estÃ¡ corriendo
curl http://localhost:8000/

# 2. Si no responde, levantar API:
uvicorn api_observable:app --reload --port 8000

# 3. Verificar puerto correcto en dashboard_live.py:
API_BASE_URL = "http://localhost:8000"  # Debe coincidir
```

---

### Problema 2: Logs No Aparecen en Archivo

**SÃ­ntoma:**
```bash
cat logs/api.log
# cat: logs/api.log: No such file or directory
```

**Causa:** ImplementaciÃ³n actual solo loggea a stdout.

**SoluciÃ³n temporal:**
```bash
# Redirigir stdout a archivo
uvicorn api_observable:app --reload --port 8000 > logs/api.log 2>&1 &
```

**SoluciÃ³n permanente (ProducciÃ³n):**
```python
# En api_observable.py, agregar FileHandler:
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    "logs/api.log",
    maxBytes=100_000_000,  # 100 MB
    backupCount=10
)
logger.logger.addHandler(handler)
```

---

### Problema 3: MÃ©tricas Se Resetean al Reiniciar API

**SÃ­ntoma:**
```bash
# Antes de restart: total_requests = 1543
# DespuÃ©s de restart: total_requests = 0
```

**Causa:** MÃ©tricas en memoria (no persisten).

**SoluciÃ³n (Desarrollo):** Aceptar limitaciÃ³n, es esperado.

**SoluciÃ³n (ProducciÃ³n):** Migrar a Prometheus (persistent storage).

---

### Problema 4: Dashboard Muestra "No endpoints with traffic yet"

**SÃ­ntoma:**
```
â±ï¸  LATENCY & ERROR RATE PER ENDPOINT
---------------------------------------------
  No endpoints with traffic yet...
```

**Causa:** No se han hecho requests a la API desde el Ãºltimo restart.

**SoluciÃ³n:**
```bash
# Generar trÃ¡fico de prueba
for i in {1..10}; do
  curl -s -X POST http://localhost:8000/api/v1/products \
    -H "Content-Type: application/json" \
    -d "{\"name\": \"Product $i\", \"price\": $((100 + i)), \"currency\": \"USD\"}" > /dev/null
done

# Ahora dashboard mostrarÃ¡ mÃ©tricas
```

---

### Problema 5: Colores No Se Ven en Windows PowerShell

**SÃ­ntoma:**
```
[91m HIGH LATENCY [0m  â† CÃ³digos ANSI visibles
```

**Causa:** PowerShell 5.1 no soporta ANSI por defecto.

**SoluciÃ³n 1: Usar Windows Terminal (recomendado)**
```powershell
# Instalar Windows Terminal desde Microsoft Store
# Ejecutar dashboard_live.py en Windows Terminal
```

**SoluciÃ³n 2: PowerShell 7+**
```powershell
# Instalar PowerShell 7:
winget install Microsoft.PowerShell

# Ejecutar dashboard en PowerShell 7
pwsh
python dashboard_live.py
```

---

## ðŸš€ PRODUCCIÃ“N

### MigraciÃ³n a Stack Production-Grade

**Recomendado: Prometheus + Grafana + Loki**

```yaml
# docker-compose.yml
services:
  api:
    build: .
    ports:
      - "8000:8000"
  
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
  
  loki:
    image: grafana/loki
    ports:
      - "3100:3100"
```

---

### Logging con Rotation

```python
# Agregar en api_observable.py
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    "logs/api.log",
    maxBytes=100_000_000,  # 100 MB
    backupCount=10         # Mantener 10 archivos = 1 GB
)
logger.logger.addHandler(handler)
```

---

### Sampling de Logs (Alto TrÃ¡fico)

```python
import random

# En observability_middleware:
sample_rate = 0.01  # 1% de trÃ¡fico exitoso

if response.status_code >= 400 or random.random() < sample_rate:
    # Solo loggear errores y 1% de requests exitosas
    logger.info("Request completed", ...)
```

---

### Alertas con Webhooks

```python
def send_slack_alert(message: str):
    import requests
    requests.post(
        "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
        json={"text": message}
    )

# En MetricsCollector:
if error_rate_5xx > 1.0:
    send_slack_alert(f"ðŸ”´ CRITICAL: {endpoint} has {error_rate_5xx}% 5xx errors")
```

---

### MÃ©tricas con Prometheus Client

```python
from prometheus_client import Histogram, Counter, generate_latest

REQUEST_LATENCY = Histogram(
    "http_request_duration_seconds",
    "HTTP request latency",
    ["method", "endpoint", "status"]
)

REQUEST_COUNT = Counter(
    "http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status"]
)

# En middleware:
REQUEST_LATENCY.labels(method=method, endpoint=endpoint, status=status).observe(latency)
REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()

# Endpoint para Prometheus scraping:
@app.get("/metrics")
def prometheus_metrics():
    return Response(content=generate_latest(), media_type="text/plain")
```

---

## ðŸ“š RECURSOS ADICIONALES

### Archivos del Proyecto
- **DASHBOARD.md:** 5 grÃ¡ficos esenciales + 3 alertas con umbrales
- **EJEMPLOS_LOGS.md:** Logs reales con anÃ¡lisis y queries jq
- **CRITICA_Y_MEJORA.md:** AnÃ¡lisis tÃ©cnico + prompt v2 production-grade

### DocumentaciÃ³n Externa
- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Four Golden Signals](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)
- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)

### Comandos Ãštiles

```bash
# Ver logs en tiempo real con filtro
tail -f logs/api.log | jq 'select(.status >= 400)'

# Contar requests por endpoint
cat logs/api.log | jq -s 'group_by(.path) | map({endpoint: .[0].path, count: length})'

# Top 10 requests mÃ¡s lentas
cat logs/api.log | jq -s 'sort_by(.latency_ms) | reverse | .[0:10] | .[] | {path, latency_ms, correlationId}'

# Ver mÃ©tricas formateadas
curl -s http://localhost:8000/api/v1/_metrics | jq '.data | to_entries[] | select(.key != "_global")'

# Generar trÃ¡fico de prueba
for i in {1..100}; do
  curl -s -X POST http://localhost:8000/api/v1/products \
    -H "Content-Type: application/json" \
    -d "{\"name\": \"Product $i\", \"price\": $((100 + i)), \"currency\": \"USD\"}" &
done
```

---

## ðŸŽ“ CONCLUSIÃ“N

Este ejercicio demuestra:

1. âœ… **Logging estructurado JSON:** Parseable, consistente, con correlationId
2. âœ… **MÃ©tricas de latencia:** P50/P95/P99 por endpoint
3. âœ… **Error rates:** SeparaciÃ³n de 4xx (cliente) y 5xx (servidor)
4. âœ… **Request tracing:** Pipeline completo con timestamps
5. âœ… **Dashboard live:** VisualizaciÃ³n en tiempo real con alertas
6. âœ… **InstrumentaciÃ³n automÃ¡tica:** Middleware centralizado (zero boilerplate)

### PrÃ³ximos Pasos

- Migrar mÃ©tricas a **Prometheus** (persistent storage)
- Agregar **RotatingFileHandler** para logs
- Implementar **OpenTelemetry** para distributed tracing
- Crear **Grafana dashboards** con historical data
- Configurar **alertas con webhooks** (Slack, PagerDuty)

---

**Â¿Preguntas o problemas?**
- Revisar **Troubleshooting** section
- Ver **EJEMPLOS_LOGS.md** para queries jq
- Consultar **CRITICA_Y_MEJORA.md** para production migration

---

**Autor:** Ejercicio 5 - Semana 7 IA  
**Licencia:** MIT (uso acadÃ©mico)  
**VersiÃ³n:** 1.0.0 (26 Nov 2025)
