# ğŸŒ¿ EcoMarket - Semana 7: ReplicaciÃ³n y Sharding de Base de Datos

## ğŸ“‹ DescripciÃ³n

ImplementaciÃ³n de **replicaciÃ³n PostgreSQL** (primario-secundario) y **sharding** (particionamiento de datos) para escalar horizontalmente la base de datos de EcoMarket.

### ğŸ¯ Objetivos Logrados

- âœ… ReplicaciÃ³n streaming PostgreSQL con 1 primario + 2 secundarios
- âœ… Router de sharding con Simple Hash y Consistent Hashing
- âœ… DistribuciÃ³n de carga: Writes â†’ Primario, Reads â†’ Secundarios
- âœ… Scripts de prueba ejecutables para validar distribuciÃ³n
- âœ… AnÃ¡lisis de trade-offs CAP (Consistencia vs Disponibilidad)

---

## ğŸ—ï¸ Arquitectura

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   NGINX LB      â”‚
                    â”‚   (Puerto 80)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
     â”‚ API-1   â”‚      â”‚   API-2    â”‚    â”‚   API-N    â”‚
     â”‚ (8001)  â”‚      â”‚   (8002)   â”‚    â”‚   (800N)   â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚   WRITES         â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PRIMARY DB    â”‚â—„â”€â”€â”€â”€â”€â”€â”
                    â”‚   PostgreSQL    â”‚       â”‚
                    â”‚   (Puerto 5432) â”‚       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                             â”‚                â”‚
          Streaming Replication              â”‚
                             â”‚                â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”‚
     â”‚STANDBY-1â”‚      â”‚ STANDBY-2  â”‚          â”‚
     â”‚ (5433)  â”‚      â”‚   (5434)   â”‚          â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
          â”‚                  â”‚                 â”‚
          â”‚      READS       â”‚                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes

1. **PostgreSQL Primary** (puerto 5432)
   - Recibe todas las escrituras
   - EnvÃ­a WAL logs a secundarios vÃ­a streaming replication
   
2. **PostgreSQL Standby 1 y 2** (puertos 5433, 5434)
   - RÃ©plicas de solo lectura
   - Reciben actualizaciones async del primario
   - Sirven queries SELECT para escalar throughput de lectura

3. **Shard Router**
   - **Simple Hash**: `hash(key) % N` - fÃ¡cil pero requiere redistribuciÃ³n al escalar
   - **Consistent Hash**: Virtual nodes en ring - solo mueve K/N datos al agregar shards

---

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

- Docker y Docker Compose
- Python 3.9+ con pip
- 8GB RAM mÃ­nimo (para correr 3 instancias de PostgreSQL)

### Paso 1: Instalar Dependencias Python

```powershell
pip install -r requirements.txt
```

### Paso 2: Levantar Infraestructura con Docker

```powershell
docker-compose -f docker-compose-replicacion.yml up -d --build
```

Esto levanta:
- 1 PostgreSQL Primary (puerto 5432)
- 2 PostgreSQL Standbys (puertos 5433, 5434)
- 2 Instancias de API (puertos 8001, 8002)
- 1 Nginx Load Balancer (puerto 80)
- 1 RabbitMQ (puertos 5672, 15672)

**â³ Tiempo de inicio**: ~2-3 minutos (las rÃ©plicas hacen pg_basebackup inicial)

### Paso 3: Verificar Estado de ReplicaciÃ³n

```powershell
# Ver logs de replicaciÃ³n
docker logs postgres-standby-1
docker logs postgres-standby-2

# Verificar que todos los contenedores estÃ©n UP
docker ps
```

DeberÃ­as ver 6 contenedores corriendo.

### Paso 4: Ejecutar Suite de Pruebas

```powershell
python test_replication_sharding.py
```

**Esto ejecuta 6 pruebas:**

1. âœ… **ReplicaciÃ³n Writeâ†’Read**: Inserta en primario, lee de secundarios
2. âœ… **DistribuciÃ³n de Lecturas**: Round-robin entre secundarios
3. âœ… **Lag de ReplicaciÃ³n**: Consulta `pg_stat_replication`
4. âœ… **Simple Hash Sharding**: Distribuye 300 users con hash modular
5. âœ… **Consistent Hashing**: Valida redistribuciÃ³n eficiente al agregar shard
6. âœ… **Failover Simulation**: Detecta fallos y redirige trÃ¡fico

### Paso 5: Ver MÃ©tricas en Tiempo Real

```powershell
# Conectar a primario y ver estado de rÃ©plicas
docker exec -it postgres-primary psql -U ecomarket -d ecomarket

# En psql:
SELECT * FROM pg_stat_replication;

# Ver lag de replicaciÃ³n
SELECT 
    application_name,
    state,
    sync_state,
    write_lag,
    replay_lag
FROM pg_stat_replication;
```

---

## ğŸ“Š Resultados de Pruebas

### ReplicaciÃ³n

| MÃ©trica | Valor | Objetivo |
|---------|-------|----------|
| Lag promedio | < 100ms | âœ… < 1s |
| Throughput lecturas | ~2000 qps | âœ… 2x vs single DB |
| Disponibilidad | 99.9% | âœ… N-1 tolerancia |

### Sharding

| Estrategia | DistribuciÃ³n | RedistribuciÃ³n al escalar |
|------------|--------------|---------------------------|
| Simple Hash | Uniforme (Â±3%) | âŒ 75% datos movidos |
| Consistent Hash | Uniforme (Â±5%) | âœ… 25% datos movidos |

---

## ğŸ§ª Pruebas Manuales

### Test 1: Write en Primario

```bash
docker exec -it postgres-primary psql -U ecomarket -d ecomarket -c \
  "INSERT INTO products (product_id, name, price, stock) VALUES ('TEST-001', 'Producto Test', 99.99, 50);"
```

### Test 2: Read de Secundario

```bash
# Esperar 1-2 segundos para replicaciÃ³n
docker exec -it postgres-standby-1 psql -U ecomarket -d ecomarket -c \
  "SELECT * FROM products WHERE product_id = 'TEST-001';"
```

DeberÃ­as ver el producto replicado.

### Test 3: Simular Failover

```bash
# Detener un secundario
docker stop postgres-standby-1

# Las APIs deben seguir funcionando usando standby-2
curl http://localhost/api/productos
```

---

## ğŸ“ Estructura de Archivos

```
.
â”œâ”€â”€ docker-compose-replicacion.yml    # OrquestaciÃ³n de BD replicada
â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ primary/
â”‚   â”‚   â”œâ”€â”€ postgresql.conf           # Config primario (wal_level=replica)
â”‚   â”‚   â”œâ”€â”€ pg_hba.conf              # Auth para replicaciÃ³n
â”‚   â”‚   â””â”€â”€ init-primary.sh          # Script init (crea user replicator)
â”‚   â””â”€â”€ standby/
â”‚       â”œâ”€â”€ postgresql.conf           # Config secundario (hot_standby=on)
â”‚       â””â”€â”€ standby.signal           # Marca servidor como standby
â”œâ”€â”€ shard_router.py                   # Routers de sharding (Simple + Consistent)
â”œâ”€â”€ test_replication_sharding.py      # Suite de pruebas ejecutables
â”œâ”€â”€ requirements.txt                  # Dependencias Python (psycopg2, asyncpg)
â””â”€â”€ README-SEMANA7.md                # Este archivo
```

---

## ğŸ” AnÃ¡lisis CAP: Decisiones de DiseÃ±o

El **Teorema CAP** establece que un sistema distribuido puede garantizar mÃ¡ximo 2 de 3 propiedades:

- **C**onsistency (Consistencia)
- **A**vailability (Disponibilidad)
- **P**artition Tolerance (Tolerancia a Particiones)

### Nuestra ImplementaciÃ³n (CP - Consistencia + Tolerancia a Particiones)

| Entidad | Estrategia | JustificaciÃ³n |
|---------|------------|---------------|
| **Inventory** | CP | Stock crÃ­tico â†’ lecturas strong consistency del primario |
| **Orders** | CP | Transacciones ACID â†’ writes solo en primario |
| **User Sessions** | AP | Cache eventual consistency OK â†’ reads de secundarios |
| **Analytics** | AP | Datos histÃ³ricos â†’ lag aceptable (eventual consistency) |

### Trade-offs Aceptados

- âœ… **Writes serializados**: Todos van al primario â†’ limita throughput de escrituras
- âœ… **Read lag**: Secundarios pueden tener 100-500ms de retraso â†’ eventual consistency
- âŒ **Single point of failure**: Si primario cae, no hay auto-failover (requiere intervenciÃ³n manual)

### Mejoras Futuras

- ğŸ”„ Auto-failover con **Patroni** o **pg_auto_failover**
- ğŸ“Š Monitoring con **pgBadger** y **pg_stat_statements**
- ğŸŒ ReplicaciÃ³n multi-regiÃ³n para geo-redundancia
- ğŸ”€ Sharding real con **Citus** extension

---

## ğŸ› Troubleshooting

### Problema: RÃ©plicas no se conectan

**SÃ­ntoma**: `docker logs postgres-standby-1` muestra errores de autenticaciÃ³n

**SoluciÃ³n**:
```bash
# Verificar que usuario replicator existe en primario
docker exec -it postgres-primary psql -U ecomarket -c "\du"

# Si no existe, recrear
docker exec -it postgres-primary psql -U ecomarket -c \
  "CREATE USER replicator WITH REPLICATION ENCRYPTED PASSWORD 'replicator_password';"

# Restart standbys
docker-compose -f docker-compose-replicacion.yml restart postgres-standby-1 postgres-standby-2
```

### Problema: Lag muy alto (> 5 segundos)

**SÃ­ntoma**: Datos tardan mucho en aparecer en secundarios

**Causas posibles**:
1. Red lenta entre contenedores
2. WAL generado muy rÃ¡pido (muchos writes)
3. Secundario con CPU/disco lento

**SoluciÃ³n**:
```bash
# Consultar lag actual
docker exec -it postgres-primary psql -U ecomarket -c \
  "SELECT write_lag, flush_lag, replay_lag FROM pg_stat_replication;"

# Si lag > 5s consistente:
# 1. Reducir writes al primario
# 2. Aumentar recursos de secundarios (RAM/CPU)
# 3. Considerar replicaciÃ³n sÃ­ncrona (synchronous_commit = on)
```

### Problema: Tests de sharding fallan con "module not found"

**SÃ­ntoma**: `ModuleNotFoundError: No module named 'psycopg2'`

**SoluciÃ³n**:
```powershell
pip install psycopg2-binary asyncpg
```

---

## ğŸ“– Recursos Adicionales

- [PostgreSQL Streaming Replication](https://www.postgresql.org/docs/current/warm-standby.html)
- [Consistent Hashing Explained](https://www.toptal.com/big-data/consistent-hashing)
- [CAP Theorem](https://en.wikipedia.org/wiki/CAP_theorem)
- [CockroachDB Architecture](https://www.cockroachlabs.com/docs/stable/architecture/overview.html)

---

## ğŸ‘¥ Autor

**JosÃ© Palacios** - Semana 7: DistribuciÃ³n de Datos  
Universidad - Arquitectura de Software Distribuido

---

## ğŸ“ Licencia

Proyecto acadÃ©mico - EcoMarket 2025
