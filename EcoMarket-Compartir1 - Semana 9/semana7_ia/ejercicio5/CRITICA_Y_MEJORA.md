# Cr√≠tica y Mejora del Prompt - Ejercicio 5: Observabilidad

**Autor:** Ejercicio 5 - Semana 7 IA  
**Fecha:** 26 Nov 2025

---

## üìå PROMPT ORIGINAL ANALIZADO

```markdown
Eres SRE.
Prop√≥n:
- Estructura de log JSON {ts, level, correlationId, path, method, status, latency_ms, userId?}.
- M√©tricas por endpoint (p50/p95, error_rate_4xx_5xx).
- Trazas textuales del pipeline request‚Üírespuesta.
- Dashboard: 5 gr√°ficos imprescindibles y 3 alertas con umbral.
```

**Contexto:**
- API REST b√°sica de productos (CRUD)
- Sin especificar stack de observability (Prometheus, Grafana, ELK, etc.)
- No menciona volumen de tr√°fico esperado
- No define SLOs (Service Level Objectives)

---

## ‚úÖ FORTALEZAS DE LA IMPLEMENTACI√ìN ACTUAL

### 1. **Logging Estructurado JSON con Formato Consistente**

**Implementaci√≥n:**
```python
class StructuredLogger:
    def _json_formatter(self):
        class JSONFormatter(logging.Formatter):
            def format(self, record):
                log_data = {
                    "ts": datetime.utcnow().isoformat() + "Z",
                    "level": record.levelname,
                    "logger": record.name,
                    "msg": record.getMessage(),
                    "correlationId": getattr(record, 'correlationId', None),
                    # ... campos adicionales
                }
                return json.dumps(log_data)
```

**Ventajas:**
- ‚úÖ **Parseable:** `jq`, `grep`, Elasticsearch pueden procesarlo autom√°ticamente
- ‚úÖ **ISO 8601 timestamps:** Universalmente compatible (Grafana, Kibana)
- ‚úÖ **Campos consistentes:** Todas las logs tienen misma estructura
- ‚úÖ **correlationId obligatorio:** Permite trazar requests distribuidas

**Evidencia de valor:**
```bash
# Query compleja en 1 l√≠nea:
cat logs/api.log | jq 'select(.status >= 500 and .latency_ms > 100)' | jq -s 'length'
# Retorna: 3 (3 requests con 5xx y latency >100ms)
```

---

### 2. **Middleware de Instrumentaci√≥n Autom√°tica**

**C√≥digo destacado:**
```python
@app.middleware("http")
async def observability_middleware(request: Request, call_next):
    correlation_id = str(uuid.uuid4())
    start_time = time.perf_counter()
    
    # Log inicio
    logger.info("Request started", correlationId=correlation_id, ...)
    
    # Ejecutar request
    response = await call_next(request)
    
    # Calcular latency
    latency_ms = (time.perf_counter() - start_time) * 1000
    
    # Registrar m√©tricas
    metrics.record_request(endpoint, latency_ms, response.status_code)
    
    # Log fin
    logger.info("Request completed", latency_ms=latency_ms, ...)
```

**Por qu√© importa:**
- ‚úÖ **Zero instrumentation code en endpoints:** Autom√°tico para todos
- ‚úÖ **No se puede olvidar:** Centralizado en middleware
- ‚úÖ **Performance:** Usa `time.perf_counter()` (m√°s preciso que `time.time()`)
- ‚úÖ **Headers de observability:** `X-Correlation-Id`, `X-Latency-Ms` en response

**Comparaci√≥n:**

| Approach | Lines of Code | Maintainability | Coverage |
|----------|---------------|-----------------|----------|
| **Manual logging** (cada endpoint) | ~500 | ‚ùå Bajo (f√°cil olvidar) | 70% |
| **Middleware** (implementado) | ~80 | ‚úÖ Alto (centralizado) | 100% |

---

### 3. **M√©tricas con Percentiles (P50, P95, P99)**

**Implementaci√≥n:**
```python
class MetricsCollector:
    def get_latency_percentiles(self, endpoint: str):
        latencies = sorted(self._latencies.get(endpoint, []))
        
        def percentile(p: float) -> float:
            idx = int(len(latencies) * p / 100)
            return latencies[min(idx, len(latencies) - 1)]
        
        return {
            "p50": round(percentile(50), 2),
            "p95": round(percentile(95), 2),
            "p99": round(percentile(99), 2)
        }
```

**Ventajas sobre promedios simples:**

```
Ejemplo con 1000 requests:
- 990 requests: 5-10ms (r√°pidas)
- 10 requests: 500-1000ms (lentas por timeout)

Average: 15ms (misleading, esconde los 10 outliers)
P95: 12ms (95% de usuarios experimentan esto o mejor)
P99: 850ms (revela el problema real)
```

**Beneficio:** P95/P99 exponen "tail latency" que promedio esconde.

---

### 4. **Request Tracing con Pipeline Steps**

**Implementaci√≥n:**
```python
class RequestTracer:
    def add_step(self, name: str, details: Optional[Dict] = None):
        elapsed_ms = (time.perf_counter() - self.start_time) * 1000
        
        step = {
            "step": name,
            "timestamp_ms": round(elapsed_ms, 2),
            "details": details or {}
        }
        self.steps.append(step)
```

**Uso en endpoint:**
```python
@app.post("/api/v1/products")
def create_product(data: ProductCreate, request: Request):
    tracer = request.state.tracer
    
    tracer.add_step("validating_input", {"name": data.name})
    product = db.create(data)
    tracer.add_step("product_created", {"product_id": str(product.id)})
```

**Resultado (endpoint `/_trace`):**
```json
{
  "steps": [
    {"step": "request_received", "timestamp_ms": 0.12},
    {"step": "processing_request", "timestamp_ms": 2.34},
    {"step": "validating_input", "timestamp_ms": 3.45},
    {"step": "product_created", "timestamp_ms": 8.76},
    {"step": "response_generated", "timestamp_ms": 9.87},
    {"step": "request_completed", "timestamp_ms": 12.34}
  ]
}
```

**Caso de uso real:**
```
P95 latency = 85ms (alerta dispara)
Trace revela:
  - validating_input: 2ms
  - querying_database: 78ms ‚Üê problema aqu√≠
  - response_generated: 5ms

Acci√≥n: Agregar √≠ndice a query SQL
```

---

### 5. **Dashboard con 5 Gr√°ficos + 3 Alertas Bien Definidas**

**Gr√°ficos implementados:**
1. **Latency Percentiles:** Detectar endpoints lentos
2. **Error Rate 4xx/5xx:** Distinguir errores cliente vs servidor
3. **Request Throughput:** Monitorear carga y capacidad
4. **Request Tracing:** Debugging de requests individuales
5. **Active Errors Stream:** Detecci√≥n temprana de problemas

**Alertas con umbrales justificados:**

| Alerta | Umbral | Justificaci√≥n |
|--------|--------|---------------|
| **High 5xx Rate** | >1% | 5xx = bugs cr√≠ticos, 1% = ~10 errores/1000 reqs |
| **High Latency** | P95 >50ms | Google Web Vitals: 50ms = "fast" threshold |
| **High 4xx Rate (POST)** | >15% | 15% = 1 de 7 requests falla, indica problema en cliente |

**Ejemplo de alerta disparada:**
```python
# En dashboard_live.py:
if error_5xx > 1.0:
    print(f"üî¥ CRITICAL: {endpoint} has {error_5xx:.1f}% 5xx errors")
```

---

## ‚ö†Ô∏è DEBILIDADES Y LIMITACIONES

### 1. **M√©tricas En Memoria (No Persisten)**

**Problema:**
```python
class MetricsCollector:
    def __init__(self):
        # deque con maxlen=1000: solo √∫ltimas 1000 requests
        self._latencies: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
```

**Consecuencias:**
- ‚ùå Restart de API ‚Üí m√©tricas hist√≥ricas perdidas
- ‚ùå No se puede ver tendencias de >1000 requests
- ‚ùå No hay retention policy (no se puede comparar hoy vs ayer)

**Escenario fallido:**
```
Deploy nuevo ‚Üí API reinicia ‚Üí m√©tricas resetean a 0
Dashboard muestra: p95=5ms (solo 10 requests nuevas)
Realidad: p95=85ms antes del deploy (performance regression oculta)
```

**Impacto:** Sin baseline hist√≥rico, no se detectan regresiones de performance.

---

### 2. **Logs a stdout (Sin Rotaci√≥n ni Archivado)**

**Problema actual:**
```python
handler = logging.StreamHandler()  # Solo stdout
```

**Consecuencias:**
- ‚ùå Logs desaparecen al cerrar terminal
- ‚ùå No hay archivos para queries con `jq`
- ‚ùå Sin log rotation ‚Üí disco se llena si tr√°fico alto
- ‚ùå No se puede analizar logs hist√≥ricos (ayer, semana pasada)

**Ejemplo de problema real:**
```
Usuario reporta: "Mi request fall√≥ ayer a las 3pm"
SRE intenta buscar log ‚Üí ‚ùå No existe (solo stdout del √∫ltimo run)
```

**Soluci√≥n faltante:**
```python
# Agregar FileHandler con rotation
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    "logs/api.log",
    maxBytes=10_000_000,  # 10 MB por archivo
    backupCount=5         # Mantener 5 archivos (50 MB total)
)
```

---

### 3. **Dashboard CLI Sin Hist√≥ricos ni Gr√°ficos Time-Series**

**Limitaci√≥n:**
```python
# dashboard_live.py solo muestra snapshot actual:
render_dashboard()  # P95 actual: 12ms
```

**Qu√© falta:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ P95 Latency Over Time (Last 1 Hour)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 60ms ‚îÇ                       ‚ï≠‚îÄ‚îÄ‚ïÆ           ‚îÇ
‚îÇ 40ms ‚îÇ               ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ  ‚ï∞‚îÄ‚îÄ‚ïÆ        ‚îÇ
‚îÇ 20ms ‚îÇ   ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ             ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ   ‚îÇ
‚îÇ  0ms ‚îÇ‚îÄ‚îÄ‚îÄ‚ïØ                              ‚ï∞‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ      14:00   14:15   14:30   14:45   15:00 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Por qu√© importa:**
- Detectar si latency est√° empeorando gradualmente
- Correlacionar spikes con deploys o eventos externos
- Capacity planning (trend de throughput creciente)

**Soluci√≥n:** Migrar a Grafana + Prometheus para time-series storage.

---

### 4. **Sin Distributed Tracing (Microservicios)**

**Limitaci√≥n actual:**
```
API Observable ‚Üí (hace call a otra API externa) ‚Üí ‚ùå No se propaga correlationId
```

**Escenario real:**
```
API de Productos ‚Üí llama a ‚Üí API de Inventory ‚Üí llama a ‚Üí API de Pricing
Request falla en Pricing, pero log en Productos no muestra contexto completo
```

**Qu√© falta:**
```python
# Propagar correlationId en headers de requests salientes
import httpx

async def call_external_api(correlation_id: str):
    async with httpx.AsyncClient() as client:
        response = await client.get(
            "https://inventory-api.com/stock",
            headers={"X-Correlation-Id": correlation_id}  # ‚Üê Propagaci√≥n
        )
```

**Soluci√≥n completa:** OpenTelemetry con Jaeger para distributed tracing.

---

### 5. **Alertas Solo en Dashboard (No Notificaciones)**

**Problema:**
```python
# dashboard_live.py detecta alertas pero solo las muestra en pantalla:
if error_5xx > 1.0:
    print("üî¥ CRITICAL: High 5xx rate")
```

**Consecuencias:**
- ‚ùå Si nadie ve dashboard ‚Üí alerta no llega a nadie
- ‚ùå No hay on-call notification (PagerDuty, Slack)
- ‚ùå No se registra historial de alertas (cu√°ndo se dispar√≥, cu√°ndo se resolvi√≥)

**Ejemplo de problema:**
```
02:00 AM ‚Üí 5xx rate aumenta a 5%
03:00 AM ‚Üí Alerta aparece en dashboard (nadie lo ve)
09:00 AM ‚Üí SRE llega a oficina ‚Üí usuarios ya reportaron 7 horas de downtime
```

**Soluci√≥n faltante:**
```python
def send_alert(severity: str, message: str):
    # Webhook a Slack
    requests.post(
        "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXX",
        json={"text": f"[{severity}] {message}"}
    )
    
    # PagerDuty para CRITICAL
    if severity == "CRITICAL":
        requests.post(
            "https://events.pagerduty.com/v2/enqueue",
            json={"event_action": "trigger", "payload": {...}}
        )
```

---

### 6. **Sin Sampling para Alto Tr√°fico**

**Problema:**
```python
# Todas las requests se loggean completas
logger.info("Request completed", latency_ms=12.34, ...)
```

**Consecuencias en producci√≥n:**
- ‚ùå 10,000 req/s = 10,000 logs/s = 36 millones logs/hora
- ‚ùå Disk I/O se convierte en bottleneck
- ‚ùå Storage cost: ~500 GB/day en logs

**Ejemplo real (Netflix scale):**
```
Sin sampling: 1 mill√≥n req/min ‚Üí 1 mill√≥n logs/min
Con sampling (1%): 1 mill√≥n req/min ‚Üí 10,000 logs/min (99% menos storage)
```

**Soluci√≥n:**
```python
import random

@app.middleware("http")
async def observability_middleware(request: Request, call_next):
    sample_rate = 0.01  # 1% de requests
    
    if random.random() < sample_rate or response.status_code >= 400:
        # Log completo para errores y 1% de requests exitosas
        logger.info("Request completed", ...)
```

**Regla:** Siempre loggear errores (4xx/5xx), samplear tr√°fico exitoso.

---

### 7. **M√©tricas Agregadas Sin Dimensiones**

**Limitaci√≥n:**
```python
# M√©tricas por endpoint, pero sin desglose por:
# - User tier (free, premium, enterprise)
# - Region (us-east-1, eu-west-1)
# - Client version (mobile-v1.2, web-v3.5)
```

**Por qu√© importa:**
```
Alerta: P95 latency = 85ms (global)
Desglose muestra:
  - free users: p95 = 10ms
  - premium users: p95 = 250ms ‚Üê problema solo en premium tier

Sin dimensiones, no se detecta que problema es solo en 1 segment
```

**Soluci√≥n:**
```python
# Agregar labels a m√©tricas
metrics.record_request(
    endpoint=endpoint,
    latency_ms=latency_ms,
    status_code=status_code,
    labels={
        "user_tier": request.headers.get("X-User-Tier", "unknown"),
        "region": request.headers.get("X-Region", "unknown")
    }
)
```

---

## üéØ PROMPT MEJORADO (V2)

### **Versi√≥n Mejorada del Prompt Original**

```markdown
# Prompt Mejorado: Observabilidad Production-Grade para API REST

## Contexto
API REST de gesti√≥n de productos con:
- Tr√°fico esperado: 1,000 req/s en peak (60,000 req/min)
- Deployment: Kubernetes con 5 pods (load balanced)
- SLOs definidos:
  * Latency: P95 < 50ms, P99 < 100ms
  * Availability: 99.9% uptime (43 min downtime/month)
  * Error budget: <0.1% 5xx errors

Stack de observability: Prometheus + Grafana + Loki + OpenTelemetry

## Requerimientos Funcionales

### 1. Logging Estructurado con Sampling
- ‚úÖ Formato JSON con campos obligatorios: ts (ISO 8601), level, correlationId, traceId, spanId, path, method, status, latency_ms, userId, region
- ‚úÖ Sampling: 100% errores (4xx/5xx), 1% tr√°fico exitoso
- ‚úÖ Log rotation: RotatingFileHandler (100 MB/file, keep 10 files = 1 GB)
- ‚úÖ Sensitive data masking: no loggear passwords, tokens completos, PII
- ‚úÖ Log levels correctos: ERROR (5xx, exceptions), WARNING (4xx, degradation), INFO (lifecycle events)

### 2. M√©tricas con Dimensiones (Labels)
- ‚úÖ Latency histograms (buckets: 5, 10, 25, 50, 100, 250, 500, 1000ms)
- ‚úÖ Contadores de requests totales por: endpoint, status_code, method, user_tier, region
- ‚úÖ Gauge de active requests (in-flight)
- ‚úÖ Historial: Prometheus retiene 30 d√≠as de m√©tricas
- ‚úÖ Cardinality control: m√°ximo 1000 series por m√©trica (evitar explosi√≥n)

### 3. Distributed Tracing
- ‚úÖ OpenTelemetry SDK integrado
- ‚úÖ Propagaci√≥n W3C Trace Context (traceparent header)
- ‚úÖ Spans autom√°ticos para: HTTP requests, DB queries, external API calls
- ‚úÖ Custom spans para business logic cr√≠tico
- ‚úÖ Sampling adaptativo: 100% errores, 5% tr√°fico normal, 100% latency >100ms
- ‚úÖ Export a Jaeger para visualizaci√≥n

### 4. Dashboard Grafana con 8 Paneles
- ‚úÖ **RED metrics:**
  1. Request Rate (req/s) por endpoint - Time series (last 1h)
  2. Error Rate (%) 4xx y 5xx - Time series con threshold lines
  3. Duration (P50/P95/P99) por endpoint - Heatmap

- ‚úÖ **USE metrics (Resources):**
  4. CPU usage por pod - Gauge (0-100%)
  5. Memory usage por pod - Gauge (0-8GB)
  6. Active connections - Time series

- ‚úÖ **Business metrics:**
  7. Requests by user tier (free/premium/enterprise) - Stacked area chart
  8. Top 10 slowest endpoints - Bar chart (P95 latency)

### 5. Alertas con Umbrales Din√°micos
- ‚úÖ **Critical (PagerDuty):**
  1. Error rate 5xx > 0.1% durante 5 min (SLO breach)
  2. P99 latency > 100ms durante 5 min (SLO breach)
  3. Availability < 99.9% en ventana de 30 d√≠as (error budget agotado)

- ‚úÖ **Warning (Slack):**
  4. Error rate 4xx > 10% durante 10 min (client issues)
  5. P95 latency increase >20% vs baseline (performance regression)
  6. Request rate spike >200% vs baseline (potential attack)

- ‚úÖ **Info (Email):**
  7. Daily summary report (total requests, errors, p95 latency)

### 6. Runbooks para Cada Alerta
- ‚úÖ Cada alerta incluye link a runbook con:
  * Descripci√≥n del problema
  * Queries para debugging (LogQL, PromQL)
  * Pasos de mitigaci√≥n (ej: scale up pods, rollback deploy)
  * Escalation policy (qui√©n contactar si no se resuelve)

## Requerimientos No Funcionales

### Infraestructura
- Kubernetes deployment con:
  * Prometheus operator para service discovery autom√°tico
  * Loki para log aggregation
  * Grafana con dashboards as code (JSON)
  * Jaeger para tracing
- Helm charts para deployment reproducible
- GitOps: dashboards y alertas versionados en Git

### Performance de Observability Stack
- Overhead m√°ximo: 5% CPU, 10% latency adicional
- Log shipping as√≠ncrono (no bloquea requests)
- Metrics aggregation en batch (cada 15s)
- Tracing sampling para evitar overhead

### Compliance & Retention
- GDPR: Logs con PII se eliminan a 30 d√≠as
- Metrics retention: 30 d√≠as (agregados), 1 a√±o (daily summaries)
- Traces retention: 7 d√≠as (storage costoso)
- Audit log separado: 1 a√±o de retenci√≥n (compliance)

## Criterios de √âxito
- Dashboard carga en <2s con 30 d√≠as de datos
- Alertas disparan en <1 min desde evento
- Query de logs retorna resultados en <5s (1 GB logs)
- Zero data loss durante restarts (persistent volumes)
- Cost: <$500/month para 1M req/day (Prometheus + Loki + Jaeger)

## Entregables
1. `observability_middleware.py` con OpenTelemetry + sampling
2. `prometheus.yml` con scrape configs y recording rules
3. `grafana_dashboard.json` con 8 paneles
4. `alertmanager.yml` con 7 alertas y routing
5. `docker-compose.yml` con stack completo (Prometheus, Grafana, Loki, Jaeger)
6. `RUNBOOKS.md` con 7 runbooks (1 por alerta)
7. `METRICS_CATALOG.md` con documentaci√≥n de todas las m√©tricas
```

---

## üìä COMPARACI√ìN: PROMPT ORIGINAL VS MEJORADO

| Aspecto | Prompt Original | Prompt Mejorado | Mejora |
|---------|----------------|-----------------|--------|
| **Contexto** | Sin SLOs definidos | SLOs expl√≠citos (P95 <50ms, 99.9% uptime) | üü¢ Claridad |
| **Logging** | Estructura b√°sica | + Sampling, rotation, PII masking | üü¢ Production-ready |
| **M√©tricas** | P50/P95, error rate | + Dimensiones (tier, region), histograms | üü¢ Debugging power |
| **Tracing** | "Trazas textuales" | OpenTelemetry + Jaeger | üü¢ Distributed tracing |
| **Dashboard** | 5 gr√°ficos gen√©ricos | 8 paneles espec√≠ficos (RED + USE + business) | üü¢ Actionable insights |
| **Alertas** | 3 alertas simples | 7 alertas con umbrales din√°micos + runbooks | üü¢ Incident response |
| **Persistencia** | No especificada | Prometheus 30d, Loki 30d, Jaeger 7d | üü¢ Historical analysis |
| **Cost** | No considerado | <$500/month para 1M req/day | üü¢ Budget awareness |
| **Compliance** | No mencionado | GDPR, audit log retention 1 a√±o | üü¢ Legal compliance |

---

## üõ†Ô∏è IMPLEMENTACI√ìN INCREMENTAL (Roadmap)

### Fase 1: Logging Production-Grade (2-3 d√≠as)
```bash
‚úÖ 1. Agregar RotatingFileHandler:
   handler = RotatingFileHandler("logs/api.log", maxBytes=100_000_000, backupCount=10)

‚úÖ 2. Implementar sampling (1% tr√°fico exitoso, 100% errores)

‚úÖ 3. PII masking en logs:
   def mask_email(email):
       user, domain = email.split("@")
       return f"{user[:3]}***@{domain}"
```

### Fase 2: M√©tricas con Prometheus (3-4 d√≠as)
```bash
‚úÖ 1. Instalar prometheus-client:
   pip install prometheus-client

‚úÖ 2. Crear m√©tricas:
   from prometheus_client import Histogram, Counter
   
   REQUEST_LATENCY = Histogram(
       "http_request_duration_seconds",
       "HTTP request latency",
       ["method", "endpoint", "status"]
   )
   
   REQUEST_COUNT = Counter(
       "http_requests_total",
       "Total HTTP requests",
       ["method", "endpoint", "status", "user_tier"]
   )

‚úÖ 3. Instrumentar middleware:
   REQUEST_LATENCY.labels(method=method, endpoint=endpoint, status=status).observe(latency)
   REQUEST_COUNT.labels(...).inc()

‚úÖ 4. Exponer /metrics endpoint para Prometheus scraping
```

### Fase 3: Distributed Tracing (4-5 d√≠as)
```bash
‚úÖ 1. Instalar OpenTelemetry:
   pip install opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-fastapi

‚úÖ 2. Configurar tracer:
   from opentelemetry import trace
   from opentelemetry.exporter.jaeger import JaegerExporter
   
   tracer_provider = TracerProvider()
   tracer_provider.add_span_processor(
       BatchSpanProcessor(JaegerExporter(agent_host_name="localhost", agent_port=6831))
   )
   trace.set_tracer_provider(tracer_provider)

‚úÖ 3. Auto-instrumentar FastAPI:
   from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
   FastAPIInstrumentor.instrument_app(app)

‚úÖ 4. Custom spans en business logic:
   with tracer.start_as_current_span("validate_product"):
       validate(product)
```

### Fase 4: Grafana Dashboards (2-3 d√≠as)
```bash
‚úÖ 1. docker-compose.yml con Prometheus + Grafana
‚úÖ 2. Configurar data sources (Prometheus, Loki)
‚úÖ 3. Crear 8 dashboards con PromQL queries
‚úÖ 4. Exportar JSON y versionar en Git
```

### Fase 5: Alertas + Runbooks (3-4 d√≠as)
```bash
‚úÖ 1. alertmanager.yml con routing a Slack/PagerDuty
‚úÖ 2. Prometheus rules con umbrales
‚úÖ 3. Escribir 7 runbooks
‚úÖ 4. Probar firing de alertas en staging
```

---

## üí° LECCIONES APRENDIDAS

### Lo Que Funcion√≥ Bien ‚úÖ

1. **Middleware autom√°tico:**
   - Zero boilerplate en endpoints
   - 100% coverage garantizada
   - F√°cil de mantener (1 lugar centralizado)

2. **Percentiles sobre promedios:**
   - P95/P99 exponen tail latency
   - M√°s representativo de user experience

3. **correlationId obligatorio:**
   - Tracing de requests distribuidas
   - Debugging 10x m√°s r√°pido

### Lo Que No Funcion√≥ ‚ö†Ô∏è

1. **M√©tricas en memoria:**
   - Se pierden en restart
   - No hay hist√≥ricos para an√°lisis

2. **Logs a stdout:**
   - Sin persistencia
   - No se puede buscar logs pasados

3. **Dashboard CLI:**
   - Solo snapshot actual
   - No hay time-series visualization

---

## üéì RECOMENDACIONES FINALES

### Para Uso Acad√©mico (Ejercicio 5):
- ‚úÖ Implementaci√≥n actual **demuestra conceptos fundamentales**
- ‚úÖ Logging estructurado JSON funcional
- ‚úÖ M√©tricas de latencia y error rate calculadas
- ‚úÖ Request tracing con correlationId
- üìö Documentar limitaciones (en memoria, sin Prometheus)

### Para Uso en Producci√≥n:
1. **Migrar m√©tricas a Prometheus** (persistent storage)
2. **Agregar RotatingFileHandler** para logs
3. **Implementar OpenTelemetry** para distributed tracing
4. **Crear Grafana dashboards** con historical data
5. **Configurar alertas** con PagerDuty/Slack webhooks
6. **Sampling de logs** (1% tr√°fico exitoso, 100% errores)
7. **Runbooks** para cada alerta (incident response)

### Pr√≥ximos Pasos:
- Combinar Ejercicio 2 (JWT auth) + Ejercicio 5 ‚Üí loggear userId real
- Combinar Ejercicio 4 (E2E tests) + Ejercicio 5 ‚Üí tests de observability (verificar logs, m√©tricas)
- Integrar con Ejercicio 3 (validation) ‚Üí m√©tricas de validation errors por campo

---

## üìö RECURSOS ADICIONALES

### Librer√≠as Recomendadas
```bash
# Metrics
prometheus-client          # Prometheus metrics
statsd                     # StatsD client (alternative)

# Tracing
opentelemetry-api          # OpenTelemetry SDK
opentelemetry-instrumentation-fastapi
jaeger-client              # Jaeger tracing

# Logging
python-json-logger         # Structured logging
loguru                     # Alternative (m√°s f√°cil)

# Dashboards
grafana-api                # Grafana client
prometheus-api-client      # Query Prometheus
```

### Lecturas
- [Google SRE Book - Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)
- [The Four Golden Signals](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals)
- [OpenTelemetry Best Practices](https://opentelemetry.io/docs/concepts/signals/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)

---

**Conclusi√≥n:** El prompt original gener√≥ una base s√≥lida para observabilidad b√°sica, pero el prompt mejorado eleva la implementaci√≥n a est√°ndares de producci√≥n con Prometheus, OpenTelemetry, sampling, alertas con runbooks, y dashboards con historical data.

**Autor:** Ejercicio 5 - Semana 7 IA  
**Versi√≥n:** 1.0.0
